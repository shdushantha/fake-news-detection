{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHt8ZbzL7XLm",
        "outputId": "cc6fe1b5-569a-4a36-cb27-1b5126ff5ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.12/dist-packages (2.0.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Spark Session created successfully!\n"
          ]
        }
      ],
      "source": [
        "######### Fake News Detection with Spark, Mlib, NLP, CNN, BERT ##################\n",
        "# Install necessary libraries\n",
        "!pip install pyspark findspark tensorflow\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"FakeNewsDetection\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAqC_bW2-hW8",
        "outputId": "eb4968f2-333c-4cd8-fcfd-a2cb38400ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fake schema:\n",
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- subject: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            "\n",
            "True schema:\n",
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- subject: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File paths (adjust accordingly)\n",
        "fake_path = \"/content/drive/MyDrive/AIMLBigData/Fake.csv\"\n",
        "true_path = \"/content/drive/MyDrive/AIMLBigData/True.csv\"\n",
        "\n",
        "#fake_path = \"Fake.csv\"\n",
        "#true_path = \"True.csv\"\n",
        "\n",
        "# Load CSVs\n",
        "df_fake = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
        "df_true = spark.read.csv(true_path, header=True, inferSchema=True)\n",
        "\n",
        "print(\"Fake schema:\")\n",
        "df_fake.printSchema()\n",
        "\n",
        "print(\"True schema:\")\n",
        "df_true.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnCnx-T1ATyC",
        "outputId": "409ff6b2-93fe-426e-ce5f-dc384038e8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records: 44906\n",
            "+--------------------+--------------------+-------+-----------------+-----+\n",
            "|               title|                text|subject|             date|label|\n",
            "+--------------------+--------------------+-------+-----------------+-----+\n",
            "| Donald Trump Sen...|Donald Trump just...|   News|December 31, 2017|    1|\n",
            "| Drunk Bragging T...|House Intelligenc...|   News|December 31, 2017|    1|\n",
            "| Sheriff David Cl...|On Friday, it was...|   News|December 30, 2017|    1|\n",
            "| Trump Is So Obse...|On Christmas day,...|   News|December 29, 2017|    1|\n",
            "| Pope Francis Jus...|Pope Francis used...|   News|December 25, 2017|    1|\n",
            "+--------------------+--------------------+-------+-----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df_fake = df_fake.withColumn(\"label\", lit(1))\n",
        "df_true = df_true.withColumn(\"label\", lit(0))\n",
        "\n",
        "df_all = df_fake.unionByName(df_true)\n",
        "print(\"Total records:\", df_all.count())\n",
        "df_all.show(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5OIc6fDAjjr",
        "outputId": "7ab084aa-6c87-494b-cebc-564843fbc6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+-------------+\n",
            "|                                                                                           full_text|numeric_label|\n",
            "+----------------------------------------------------------------------------------------------------+-------------+\n",
            "| donald trump sends out embarrassing new years eve message this is disturbing donald trump just c...|          0.0|\n",
            "| drunk bragging trump staffer started russian collusion investigation house intelligence committe...|          0.0|\n",
            "| sheriff david clarke becomes an internet joke for threatening to poke people in the eye on frida...|          0.0|\n",
            "| trump is so obsessed he even has obamas name coded into his website images on christmas day dona...|          0.0|\n",
            "| pope francis just called out donald trump during his christmas speech pope francis used his annu...|          0.0|\n",
            "+----------------------------------------------------------------------------------------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Preprocess the new contents ###\n",
        "from pyspark.sql.functions import concat_ws, lower, regexp_replace, col\n",
        "\n",
        "# Combine title and text into a single 'content' column\n",
        "df_all = df_all.withColumn(\"full_text\", concat_ws(\" \", col(\"title\"), col(\"text\")))\n",
        "\n",
        "# Define a common label indexer\n",
        "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"numeric_label\")\n",
        "# Apply the label indexer to the DataFrame\n",
        "df_all = label_indexer.fit(df_all).transform(df_all)\n",
        "\n",
        "\n",
        "# Basic text cleaning: lowercase, remove punctuation, digits, extra spaces\n",
        "df_all = df_all.withColumn(\"full_text\", lower(col(\"full_text\")))\n",
        "df_all = df_all.withColumn(\"full_text\", regexp_replace(col(\"full_text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "df_all = df_all.withColumn(\"full_text\", regexp_replace(col(\"full_text\"), \"\\\\s+\", \" \"))\n",
        "\n",
        "df_all.select(\"full_text\", \"numeric_label\").show(5, truncate=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH0e-4vI5m_b",
        "outputId": "96a392c5-6d74-4840-9294-44250ba18da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|numeric_label|count|\n",
            "+-------------+-----+\n",
            "|          0.0|23489|\n",
            "|          1.0|21417|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# group by numeric_label\n",
        "df_all.groupBy(\"numeric_label\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ2cDFh454nA",
        "outputId": "4f343f81-7257-4a88-967b-1c10145737f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set count: 36086\n",
            "Test set count: 8820\n"
          ]
        }
      ],
      "source": [
        "# Train test split\n",
        "train_df, test_df = df_all.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Train set count:\", train_df.count())\n",
        "print(\"Test set count:\", test_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0j-EgOr6b6M",
        "outputId": "846f0ec9-2715-497e-fa8e-cd3596af28f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MLlib Logistic Regression model...\n",
            "‚úÖ Baseline Model (Logistic Regression) AUC: 0.9982\n"
          ]
        }
      ],
      "source": [
        "# Define the ML Pipeline stages for Logistic Regression\n",
        "tokenizer = Tokenizer(inputCol=\"full_text\", outputCol=\"words\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"numeric_label\")\n",
        "\n",
        "# Assemble and train the pipeline\n",
        "pipeline_lr = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, lr])\n",
        "print(\"Training MLlib Logistic Regression model...\")\n",
        "lr_model = pipeline_lr.fit(train_df)\n",
        "\n",
        "# Make predictions on the test set\n",
        "# We will use these predictions in the final ensemble step\n",
        "lr_predictions = lr_model.transform(test_df)\n",
        "\n",
        "# Evaluate its standalone performance\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"numeric_label\")\n",
        "auc = evaluator.evaluate(lr_predictions)\n",
        "print(f\"‚úÖ Baseline Model (Logistic Regression) AUC: {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "SPK7zoM38Eqg",
        "outputId": "5c55434f-60cd-4a30-9e4a-277753015945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning BERT model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1128' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1128/1128 04:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.027990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>0.014849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.010460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.009573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.010700</td>\n",
              "      <td>0.007328</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions from BERT model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BERT model trained and predictions are ready.\n"
          ]
        }
      ],
      "source": [
        "# --- Part 1: Prepare Data for BERT ---\n",
        "# Convert the Spark test and train dataframes to Pandas\n",
        "train_pdf = train_df.select(\"full_text\", \"numeric_label\").toPandas()\n",
        "test_pdf = test_df.select(\"full_text\", \"numeric_label\").toPandas()\n",
        "\n",
        "# Map labels to integers\n",
        "# Correct the label mapping to handle numeric labels from Spark\n",
        "label_map = {0.0: 0, 1.0: 1}\n",
        "train_pdf['label_id'] = train_pdf['numeric_label'].map(label_map)\n",
        "test_pdf['label_id'] = test_pdf['numeric_label'].map(label_map)\n",
        "\n",
        "\n",
        "train_texts = train_pdf['full_text'].tolist()\n",
        "train_labels = train_pdf['label_id'].tolist()\n",
        "test_texts = test_pdf['full_text'].tolist()\n",
        "test_labels = test_pdf['label_id'].tolist()\n",
        "\n",
        "\n",
        "# --- Part 2: BERT Tokenization ---\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, IntervalStrategy # Changed to DistilBert\n",
        "\n",
        "#MODEL_NAME = 'bert-base-uncased'\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME) # Changed to DistilBertTokenizer\n",
        "\n",
        "# Tokenize the datasets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Create a PyTorch dataset class\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Ensure labels are long tensors\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "# --- Part 3: Fine-Tune the Model ---\n",
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2) # Changed to DistilBert\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directory to save the model\n",
        "\n",
        "    # --- Performance Optimizations ---\n",
        "    fp16=True,                       # ‚úÖ Use mixed-precision for a 30-50% speedup on GPU\n",
        "    per_device_train_batch_size=32,  # ‚úÖ Double the batch size (possible due to fp16)\n",
        "\n",
        "    # --- Training Strategy ---\n",
        "    num_train_epochs=1,              # 1 epoch is often sufficient for fine-tuning\n",
        "    learning_rate=2e-5,              # A standard learning rate for fine-tuning BERT\n",
        "    warmup_steps=500,                # Standard practice\n",
        "    weight_decay=0.01,               # Standard practice for regularization\n",
        "\n",
        "    # --- Monitoring and Saving ---\n",
        "    eval_strategy=IntervalStrategy.STEPS,     # üí° Evaluate performance during training\n",
        "    eval_steps=200,                  # üí° Evaluate every 200 steps\n",
        "    save_strategy=IntervalStrategy.STEPS,           # üí° Match the save strategy to evaluation\n",
        "    save_steps=200,                  # üí° Save a checkpoint every 200 steps\n",
        "    # load_best_model_at_end=True,     # üí° Automatically load the best model at the end (unsupported in this version)\n",
        "\n",
        "    # --- Logging ---\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,                # Log more frequently to see progress\n",
        "    report_to=\"none\",                # Keep this to disable external logging like wandb\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning BERT model...\")\n",
        "trainer.train()\n",
        "\n",
        "# --- Part 4: Get Predictions from BERT ---\n",
        "print(\"Getting predictions from BERT model...\")\n",
        "bert_raw_predictions, _, _ = trainer.predict(test_dataset)\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "from scipy.special import softmax\n",
        "bert_probabilities = softmax(bert_raw_predictions, axis=1)\n",
        "\n",
        "# We are interested in the probability of the 'FAKE' class (which we mapped to 1)\n",
        "bert_probs_fake = bert_probabilities[:, 1]\n",
        "print(\"‚úÖ BERT model trained and predictions are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save the trained models ---\n",
        "\n",
        "# Save the Spark MLlib Logistic Regression model\n",
        "lr_model_path = \"/content/drive/MyDrive/AIMLBigData/lr_model\"\n",
        "# Use write().overwrite().save() to overwrite the existing model\n",
        "lr_model.write().overwrite().save(lr_model_path)\n",
        "print(f\"Logistic Regression model saved to: {lr_model_path}\")\n",
        "\n",
        "# Save the Hugging Face Transformers BERT model\n",
        "bert_model_path = \"/content/drive/MyDrive/AIMLBigData/bert_model\"\n",
        "# You might need to manually remove the directory if it exists before saving\n",
        "# or implement a check and remove logic if overwriting is strictly needed\n",
        "# For this example, we assume overwrite() for Spark is sufficient if BERT save works.\n",
        "# If BERT save fails due to existing files, you might need os.makedirs(bert_model_path, exist_ok=True)\n",
        "# or a more robust overwrite mechanism depending on the exact error.\n",
        "model.save_pretrained(bert_model_path)\n",
        "tokenizer.save_pretrained(bert_model_path) # Save the tokenizer along with the model\n",
        "print(f\"BERT model saved to: {bert_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBKAxZUpYkMh",
        "outputId": "247b02ae-d3fa-4e2b-f52e-c441a76bc017"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model saved to: /content/drive/MyDrive/AIMLBigData/lr_model\n",
            "BERT model saved to: /content/drive/MyDrive/AIMLBigData/bert_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 1: Extract Probabilities from Logistic Regression ---\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# The 'probability' column is a Vector. We need to extract the probability of the positive class (index 1).\n",
        "# Note: Ensure your label_indexer mapped 'FAKE' to 1.0. If not, adjust the index.\n",
        "extract_prob_udf = udf(lambda v: float(v[1]), FloatType())\n",
        "lr_predictions_with_prob = lr_predictions.withColumn(\"lr_prob_fake\", extract_prob_udf(\"probability\"))\n",
        "\n",
        "# Collect the LR probabilities into a pandas Series to easily combine them\n",
        "lr_probs_fake_pdf = lr_predictions_with_prob.select(\"lr_prob_fake\").toPandas()\n",
        "\n",
        "\n",
        "# --- Part 2: Combine and Evaluate the Ensemble ---\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# Create a final DataFrame for evaluation\n",
        "# This assumes the order of test_data was preserved, which it should be in this workflow.\n",
        "ensemble_df = pd.DataFrame({\n",
        "    'true_label': test_labels,\n",
        "    'lr_prob': lr_probs_fake_pdf['lr_prob_fake'],\n",
        "    'bert_prob': bert_probs_fake\n",
        "})\n",
        "\n",
        "# Define weights\n",
        "bert_weight = 0.80\n",
        "lr_weight = 0.20\n",
        "\n",
        "# Calculate the weighted average probability\n",
        "ensemble_df['ensemble_prob'] = (bert_weight * ensemble_df['bert_prob']) + \\\n",
        "                               (lr_weight * ensemble_df['lr_prob'])\n",
        "\n",
        "# Determine final prediction based on a 0.5 threshold\n",
        "ensemble_df['ensemble_prediction'] = (ensemble_df['ensemble_prob'] > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# --- Part 3: Final Evaluation ---\n",
        "ensemble_accuracy = accuracy_score(ensemble_df['true_label'], ensemble_df['ensemble_prediction'])\n",
        "ensemble_auc = roc_auc_score(ensemble_df['true_label'], ensemble_df['ensemble_prob'])\n",
        "\n",
        "print(\"\\n--- Final Ensemble Results ---\")\n",
        "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
        "print(f\"Ensemble AUC: {ensemble_auc:.4f}\")"
      ],
      "metadata": {
        "id": "qVskJP0QVIuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d03fc79-dc19-4a98-fc21-e85a302c3e28"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Ensemble Results ---\n",
            "Ensemble Accuracy: 0.9989\n",
            "Ensemble AUC: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "# No need to import StringIndexerModel here as we use the fitted object directly\n",
        "\n",
        "# Ensure the BERT model is in evaluation mode\n",
        "# In the Trainer workflow, this is handled automatically during predict,\n",
        "# but for manual inference, it's good practice.\n",
        "model.eval()\n",
        "\n",
        "# Determine the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Move the model to the determined device\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Let's define the weights again for clarity\n",
        "bert_weight = 0.80\n",
        "lr_weight = 0.20\n",
        "\n",
        "# The numeric label 1.0 corresponds to the 'FAKE' class based on how the data was labeled.\n",
        "# Both the Logistic Regression probability vector and the BERT output logits\n",
        "# are ordered according to the numeric labels (0.0 then 1.0).\n",
        "# So, the probability of FAKE is at index 1.\n",
        "FAKE_PROB_INDEX = 1\n",
        "print(f\"Index for 'FAKE' probability is: {FAKE_PROB_INDEX}\")\n",
        "\n",
        "\n",
        "def predict_news(news_text: str):\n",
        "    \"\"\"\n",
        "    Takes a news article string and classifies it as 'Fake News' or 'True News'\n",
        "    using the trained LR + BERT ensemble model.\n",
        "    \"\"\"\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(f\"Analyzing news: '{news_text[:100]}...'\")\n",
        "\n",
        "    # === 1. Get Logistic Regression Prediction ===\n",
        "    # Create a Spark DataFrame with the new text\n",
        "    sample_df = spark.createDataFrame([(news_text,)], [\"full_text\"])\n",
        "\n",
        "    # Get the prediction from the saved Spark ML Pipeline\n",
        "    lr_pred = lr_model.transform(sample_df)\n",
        "\n",
        "    # Extract the probability of the \"FAKE\" class\n",
        "    # Use the FAKE_PROB_INDEX (which is 1, corresponding to numeric_label 1.0)\n",
        "    prob_lr = lr_pred.select(\"probability\").first().probability[FAKE_PROB_INDEX]\n",
        "    print(f\"üîé Logistic Regression confidence (Fake): {prob_lr:.4f}\")\n",
        "\n",
        "    # === 2. Get BERT Prediction ===\n",
        "    # Tokenize the text for BERT\n",
        "    inputs = tokenizer(news_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "\n",
        "    # Make prediction (no gradients needed for inference)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Move logits back to CPU for softmax and numpy conversion\n",
        "    logits = logits.cpu()\n",
        "\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = softmax(logits.numpy(), axis=1)[0]\n",
        "    # The BERT model was trained with numeric_label 0 as True and 1 as Fake.\n",
        "    # So, the probability of FAKE is at index 1.\n",
        "    prob_bert = probabilities[1] # Index 1 for FAKE probability\n",
        "    print(f\"ü§ñ BERT confidence (Fake): {prob_bert:.4f}\")\n",
        "\n",
        "    # === 3. Calculate Ensemble Result ===\n",
        "    final_prob = (bert_weight * prob_bert) + (lr_weight * prob_lr)\n",
        "    print(f\"‚öñÔ∏è Final Weighted Ensemble confidence (Fake): {final_prob:.4f}\")\n",
        "\n",
        "    # === 4. Final Verdict ===\n",
        "    if final_prob > 0.5:\n",
        "        verdict = \"Fake News\"\n",
        "    else:\n",
        "        verdict = \"True News\"\n",
        "\n",
        "    confidence = final_prob if verdict == \"Fake News\" else 1 - final_prob\n",
        "\n",
        "    print(f\"\\n‚úÖ FINAL VERDICT: ** {verdict} ** (Confidence: {confidence:.2%})\")\n",
        "    print(\"-----------------------------------------\")\n",
        "    return verdict, confidence\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# Example 1: A potentially fake news headline\n",
        "fake_news_sample = \"BREAKING: Scientists Discover Unicorns Living in a Hidden Valley in the Andes, Government Trying to Cover it Up.\"\n",
        "predict_news(fake_news_sample)\n",
        "\n",
        "# Example 2: A more plausible, true-sounding news headline\n",
        "true_news_sample = \"The Federal Reserve announced today it would hold interest rates steady, citing moderate economic growth and stable inflation figures.\"\n",
        "predict_news(true_news_sample)\n",
        "\n",
        "# Example 3: A tricky, politically charged example\n",
        "tricky_news_sample = \"Sources inside the White House claim a new secret tax on gasoline is being planned, set to be announced next month without public debate.\"\n",
        "predict_news(tricky_news_sample)"
      ],
      "metadata": {
        "id": "ThWImCq9VZWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0cfd6be-03d3-4903-d91e-dd359afe974d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Index for 'FAKE' probability is: 1\n",
            "-----------------------------------------\n",
            "Analyzing news: 'BREAKING: Scientists Discover Unicorns Living in a Hidden Valley in the Andes, Government Trying to ...'\n",
            "üîé Logistic Regression confidence (Fake): 0.0000\n",
            "ü§ñ BERT confidence (Fake): 0.0339\n",
            "‚öñÔ∏è Final Weighted Ensemble confidence (Fake): 0.0272\n",
            "\n",
            "‚úÖ FINAL VERDICT: ** True News ** (Confidence: 97.28%)\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "Analyzing news: 'The Federal Reserve announced today it would hold interest rates steady, citing moderate economic gr...'\n",
            "üîé Logistic Regression confidence (Fake): 0.0002\n",
            "ü§ñ BERT confidence (Fake): 0.8391\n",
            "‚öñÔ∏è Final Weighted Ensemble confidence (Fake): 0.6713\n",
            "\n",
            "‚úÖ FINAL VERDICT: ** Fake News ** (Confidence: 67.13%)\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "Analyzing news: 'Sources inside the White House claim a new secret tax on gasoline is being planned, set to be announ...'\n",
            "üîé Logistic Regression confidence (Fake): 0.0000\n",
            "ü§ñ BERT confidence (Fake): 0.9734\n",
            "‚öñÔ∏è Final Weighted Ensemble confidence (Fake): 0.7787\n",
            "\n",
            "‚úÖ FINAL VERDICT: ** Fake News ** (Confidence: 77.87%)\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Fake News', np.float64(0.7787147184762111))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}